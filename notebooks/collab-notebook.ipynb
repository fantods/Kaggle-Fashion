{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle\n",
    "!mkdir .kaggle\n",
    "!touch .kaggle/kaggle.json \n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "# add kaggle_creds to kaggle.json\n",
    "!kaggle competitions download -c imaterialist-challenge-fashion-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip /content/.kaggle/competitions/imaterialist-challenge-fashion-2018/test.json.zip -d data/\n",
    "!unzip /content/.kaggle/competitions/imaterialist-challenge-fashion-2018/train.json.zip -d data/\n",
    "!unzip /content/.kaggle/competitions/imaterialist-challenge-fashion-2018/validation.json.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import threading\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, GlobalAveragePooling2D, GlobalMaxPooling2D, MaxPooling2D\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "DATA_DIR = \"data/\"\n",
    "NUM_CLASSES = 228\n",
    "IMAGE_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-071ce51b0371>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"train.json\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"test.json\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"validation.json\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtrain_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtest_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvalidation_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train.json'"
     ]
    }
   ],
   "source": [
    "with open(DATA_DIR + \"train.json\") as train, open(DATA_DIR + \"test.json\") as test, open(DATA_DIR + \"validation.json\") as validation:\n",
    "    train_json = json.load(train)\n",
    "    test_json = json.load(test)\n",
    "    validation_json = json.load(validation)\n",
    "    \n",
    "train_urls = [obj['url'] for obj in train_json['images']]\n",
    "test_urls = [obj['url'] for obj in test_json['images']]\n",
    "validation_urls = [obj['url'] for obj in validation_json['images']]\n",
    "\n",
    "print(train_urls[:3])\n",
    "print(test_urls[:3])\n",
    "print(validation_urls[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_array(json_obj):\n",
    "    result = []\n",
    "    for data in json_obj['annotations']:\n",
    "        temp_array = [0] * NUM_CLASSES\n",
    "        for elem in data['labelId']:\n",
    "            temp_array[int(elem) - 1] = 1\n",
    "        result.append(temp_array)\n",
    "    return np.array(result)\n",
    "\n",
    "train_labels = generate_label_array(train_json)\n",
    "validation_labels = generate_label_array(validation_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SIZE=(IMAGE_SIZE,IMAGE_SIZE)\n",
    "\n",
    "rand_img = np.random.randint(0, len(train_urls))\n",
    "img_label = np.array(train_labels[rand_img]).reshape(1, 228)\n",
    "img_path = train_urls[rand_img]\n",
    "img_file = urlopen(img_path)\n",
    "image = Image.open(img_file)\n",
    "image_resized = image.resize(TARGET_SIZE, Image.ANTIALIAS)\n",
    "image_resized.thumbnail(TARGET_SIZE, Image.ANTIALIAS)\n",
    "plt.imshow(np.asarray(image_resized))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSequence(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        images = [self.url_to_image(file_name) for file_name in batch_x]\n",
    "        return np.array(images), np.array(batch_y)\n",
    "    \n",
    "    def url_to_image(self, url):\n",
    "        try:\n",
    "            resp = urllib.urlopen(url)\n",
    "            image = np.asarray(bytearray(resp.read()), dtype='uint8')\n",
    "            image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "        except:\n",
    "            output = [1]*(IMAGE_SIZE*IMAGE_SIZE*3)\n",
    "            output = np.array(output).reshape(IMAGE_SIZE,IMAGE_SIZE,3).astype('uint8')\n",
    "            image = Image.fromarray(output).convert('RGB')\n",
    "        # convert image from BGR to RGB\n",
    "        image = image[...,::-1]\n",
    "        image = np.array(image)\n",
    "        image = resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3),\n",
    "    include_top = False,\n",
    "    classes = NUM_CLASSES\n",
    ")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(GlobalMaxPooling2D())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(30, activation = 'relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "conv_base.trainable = False\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset the images since it takes forever\n",
    "train_urls = train_urls[:200000]\n",
    "train_labels = train_labels[:200000]\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH = 128\n",
    "STEPS = len(train_urls) // BATCH\n",
    "VAL_STEPS = len(validation_urls) // BATCH\n",
    "\n",
    "train_gen = BatchSequence(train_urls, train_labels, BATCH)\n",
    "val_gen = BatchSequence(validation_urls, validation_labels, BATCH)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer='rmsprop', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='model.best.hdf5', \n",
    "    verbose=1,\n",
    "    save_best_only=True\n",
    ")\n",
    "\n",
    "history = model.fit_generator(\n",
    "    generator = train_gen,\n",
    "    validation_data = val_gen,\n",
    "    epochs = EPOCHS,\n",
    "    steps_per_epoch = STEPS,\n",
    "    callbacks = [checkpointer],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
