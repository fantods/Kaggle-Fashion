{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matt\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from keras.utils.data_utils import Sequence\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Dense, Dropout, Flatten, Convolution2D, GlobalAveragePooling2D, GlobalMaxPooling2D, MaxPooling2D\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "NUM_CLASSES = 228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/test/1.jpg', '../data/test/2.jpg', '../data/test/3.jpg']\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_DIR + \"test.json\") as test:\n",
    "    test_json = json.load(test)\n",
    "    \n",
    "# test_urls = [obj['url'] for obj in test_json['images']]\n",
    "test_paths = [\"../data/test/{}.jpg\".format(obj['imageId']) for obj in test_json['images']]\n",
    "print(test_paths[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE_SIZE = 100\n",
    "# conv_base = applications.Xception(weights = \"imagenet\", include_top=False, input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "# for layer in conv_base.layers[:3]:\n",
    "#     layer.trainable = False\n",
    "# model = Sequential()\n",
    "# model.add(conv_base)\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dropout(0.4))\n",
    "# model.add(Dense(1024, activation='relu'))\n",
    "# model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "# model.load_weights(DATA_DIR + \"model.best.100.hdf5\")\n",
    "\n",
    "# model.compile(\n",
    "#     loss = \"categorical_crossentropy\", \n",
    "#     optimizer = optimizers.SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False),\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "IMAGE_SIZE = 256\n",
    "conv_base = applications.Xception(weights = \"imagenet\", include_top=False, input_shape = (IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "for layer in conv_base.layers[:3]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "model.load_weights(DATA_DIR + \"model.xcept.256.hdf5\")\n",
    "model.compile(\n",
    "    loss = \"categorical_crossentropy\", \n",
    "    optimizer = optimizers.SGD(lr=0.01, momentum=0.9), \n",
    "    # optimizer = optimizers.SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBatchSequence(Sequence):\n",
    "    def __init__(self, x_set, batch_size, resize = False):\n",
    "        self.x = x_set\n",
    "        self.batch_size = batch_size\n",
    "        self.resize = resize\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        images = np.empty([len(batch_x), IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "        for i, path in enumerate(batch_x):\n",
    "            try:\n",
    "                if self.resize:\n",
    "                    img = Image.open(path)\n",
    "                    img.thumbnail((IMAGE_SIZE, IMAGE_SIZE))\n",
    "                    image = np.array(img)\n",
    "                else:\n",
    "                    image = np.array(Image.open(path))\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                output = [1]*(IMAGE_SIZE*IMAGE_SIZE*3)\n",
    "                output = np.array(output).reshape(IMAGE_SIZE,IMAGE_SIZE,3).astype('uint8')\n",
    "                image = Image.fromarray(output).convert('RGB')\n",
    "            images[i, ...] = image\n",
    "        return images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621/621 [==============================] - ETA: 5: - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 58 - ETA: 58 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 257s 413ms/step\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "BATCH = 64\n",
    "STEPS = len(test_paths) // BATCH\n",
    "\n",
    "# test_seq = TestBatchSequence(test_paths, BATCH, resize = True)\n",
    "test_seq = TestBatchSequence(test_paths, BATCH, resize = False)\n",
    "\n",
    "probs = model.predict_generator(\n",
    "    test_seq,\n",
    "    steps = STEPS + 1,\n",
    "    workers = 5,\n",
    "    verbose = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.54554935e-07, 3.03342147e-03, 7.86876917e-05, 2.59941327e-04,\n",
       "       2.38500186e-04, 2.45332543e-04, 3.80298519e-03, 6.70182999e-05,\n",
       "       8.43210437e-04, 9.98339310e-05, 3.41876759e-04, 2.61510140e-05,\n",
       "       5.08958357e-04, 9.29917616e-04, 1.03101972e-03, 6.50179345e-07,\n",
       "       5.79797141e-02, 7.60151260e-03, 2.52699610e-02, 2.21377555e-02,\n",
       "       1.20388031e-04, 2.40078334e-05, 3.49973106e-06, 3.26875374e-06,\n",
       "       5.05328586e-04, 8.35255429e-04, 1.50950027e-05, 1.19565765e-03,\n",
       "       1.00498983e-05, 2.36800843e-04, 1.14327238e-04, 1.29744841e-03,\n",
       "       1.55035945e-04, 1.14679184e-04, 1.81504729e-04, 1.18211051e-02,\n",
       "       7.27815146e-04, 6.88128930e-04, 2.05254721e-04, 4.08001215e-04,\n",
       "       9.92947434e-07, 2.22320508e-04, 1.54077570e-04, 1.85071621e-02,\n",
       "       8.29508936e-05, 1.22713823e-06, 9.92098940e-04, 6.40111801e-04,\n",
       "       1.75127145e-02, 1.38331134e-05, 7.10855646e-04, 6.17460173e-04,\n",
       "       2.33984906e-02, 8.55926846e-05, 7.09463900e-04, 5.11310122e-04,\n",
       "       5.29343488e-05, 1.33374109e-04, 7.44385133e-03, 1.29647058e-04,\n",
       "       4.01617726e-03, 2.24399734e-02, 2.13097665e-03, 2.52426689e-04,\n",
       "       1.00438343e-03, 4.65766340e-02, 3.01510900e-05, 3.08208178e-06,\n",
       "       5.92102937e-04, 9.26014036e-03, 5.49545337e-04, 7.33010471e-04,\n",
       "       4.92878817e-03, 1.01408316e-03, 2.68710137e-04, 2.31150943e-06,\n",
       "       3.99492856e-04, 7.29285320e-03, 1.30878976e-02, 8.34028324e-05,\n",
       "       3.86659114e-04, 5.63263777e-04, 3.45435274e-06, 9.29078510e-07,\n",
       "       1.65572219e-05, 1.17113223e-06, 3.18406918e-03, 2.94949324e-03,\n",
       "       1.46447623e-04, 2.12549276e-05, 6.32346934e-03, 7.22219760e-04,\n",
       "       7.45823665e-04, 4.79261143e-06, 3.26933223e-03, 1.14092567e-04,\n",
       "       4.14000591e-03, 2.17820965e-02, 1.77045760e-03, 2.22387537e-03,\n",
       "       1.05803553e-03, 2.96757295e-04, 7.60298048e-04, 1.76966103e-06,\n",
       "       7.07171559e-02, 3.86744887e-02, 9.50624951e-07, 1.95638495e-06,\n",
       "       4.97910893e-04, 5.28127514e-03, 3.17692262e-04, 2.13794065e-05,\n",
       "       1.96303297e-02, 7.99536065e-04, 3.43669532e-03, 1.10085141e-02,\n",
       "       1.30173820e-03, 1.62637341e-04, 1.03942712e-05, 1.92621854e-04,\n",
       "       3.43201420e-04, 5.37040411e-03, 7.78005870e-06, 4.32715069e-06,\n",
       "       1.61628050e-05, 2.73382495e-04, 1.65087244e-04, 6.36032689e-03,\n",
       "       1.46131188e-05, 1.36147763e-04, 9.86153353e-03, 5.27415075e-04,\n",
       "       1.25806239e-02, 1.28255570e-05, 7.76913075e-04, 6.80807978e-04,\n",
       "       2.17792597e-02, 2.15664860e-02, 4.71564825e-04, 5.39893517e-04,\n",
       "       2.05728982e-04, 4.66727093e-03, 1.70553126e-03, 2.63216277e-03,\n",
       "       8.62635829e-07, 7.66056910e-05, 4.40058211e-04, 2.41257381e-02,\n",
       "       2.57429783e-05, 6.47207722e-04, 5.11581218e-03, 1.70271567e-04,\n",
       "       4.74250540e-02, 6.18570717e-03, 3.44384555e-03, 8.24582457e-07,\n",
       "       3.03672437e-06, 1.15076499e-03, 1.64750509e-03, 4.67157552e-05,\n",
       "       1.40185966e-06, 9.01757119e-07, 7.20070830e-07, 2.10160799e-02,\n",
       "       1.45947328e-03, 2.32142606e-03, 1.22712378e-03, 1.83003824e-04,\n",
       "       4.80420946e-04, 6.09968277e-03, 5.59202731e-02, 2.58375098e-06,\n",
       "       9.31892009e-06, 2.90692378e-06, 9.05062351e-03, 1.19612189e-02,\n",
       "       3.59471378e-05, 4.46951890e-04, 3.51820313e-06, 8.17336049e-03,\n",
       "       2.34935596e-03, 5.30293146e-05, 1.67823583e-03, 1.41579648e-02,\n",
       "       1.91346902e-04, 1.91144031e-02, 2.82557303e-04, 1.12608650e-04,\n",
       "       4.15223883e-03, 5.71790477e-03, 4.73521732e-06, 1.60355412e-05,\n",
       "       3.90479108e-03, 4.66546908e-06, 1.95223147e-05, 1.01303674e-04,\n",
       "       1.75463156e-05, 8.18895023e-06, 2.67849318e-05, 4.15430812e-04,\n",
       "       5.25230658e-04, 4.40345975e-05, 1.68096810e-03, 3.75088095e-03,\n",
       "       6.33573579e-03, 8.89319781e-05, 1.89522245e-06, 1.24668586e-05,\n",
       "       4.74149165e-05, 1.02940551e-03, 1.80588249e-05, 4.14430455e-04,\n",
       "       2.40614114e-04, 4.71547544e-02, 1.03667753e-06, 3.99548095e-04,\n",
       "       5.73201047e-04, 5.58595220e-03, 7.95385495e-06, 1.51702028e-03,\n",
       "       3.35636969e-06, 3.97873968e-02, 2.25369536e-06, 1.61626050e-03,\n",
       "       6.27547153e-04, 3.73261236e-03, 5.38285123e-04, 4.37604358e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17, 66, 105, 153, 171, 214], [17, 105, 106, 153, 171, 214, 222], [17, 105, 153, 171, 214, 222], [17, 105, 153, 171, 214, 222], [17, 66, 105, 153, 171, 214]]\n"
     ]
    }
   ],
   "source": [
    "def generate_prob_labels(probas):\n",
    "    label_preds = []\n",
    "    for i in range(len(probas)):\n",
    "        labels = []\n",
    "        proba = list(probas[i])\n",
    "        for i, elem in enumerate(proba):\n",
    "            if elem > 0.04:\n",
    "                labels.append(i + 1)\n",
    "        label_preds.append(labels)\n",
    "    return label_preds\n",
    "\n",
    "probas = generate_prob_labels(probs)\n",
    "print(probas[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff71d3603cb641c98d5371d7b0e3d551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=39706), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"submission-new.csv\",\"w\") as f:\n",
    "    f.write(\"image_id,label_id\\n\")\n",
    "    for i, labels in tqdm(enumerate(probas), total = len(probas)):\n",
    "        output_labels = \" \".join(str(x) for x in labels)\n",
    "        f.write(\"{},{}\\n\".format(i + 1, output_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
